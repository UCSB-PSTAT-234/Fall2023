{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab8.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8\n",
    "\n",
    "**Note: Gradescope autograder will not work with this notebook because of the difference in how command line commands run in your notebook vs. on Gradescope. You will rely soley on the built-in notebook for feedback.**\n",
    "\n",
    "**Because this, Lab 9 will not be graded; however, you are expected to know the material**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 1a: Downloading from URL\n",
    "\n",
    "So far, we used `wget` to download from URLs. There is another command, `curl`, that also downloads from URLs. In this assignment, we will use `curl` and other command line tools to scrape [annual air quality summary data from EPA](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Annual).\n",
    "\n",
    "![aqi-chart](images/aqi-chart.png)\n",
    "\n",
    "* Visit and [inspect the source code](https://www.lifewire.com/view-web-source-code-4151702) of this EPA webpage: https://aqs.epa.gov/aqsweb/airdata/download_files.html\n",
    "* Use `curl` command to download the same URL and [save the output to a file using `>` redirect](http://swcarpentry.github.io/shell-novice/04-pipefilter/index.html). Name the output file `files.html`\n",
    "* Use `tail` to print 10 last lines of `files.html`\n",
    "\n",
    "_Use `!` or `%%bash` when running any shell command inside the notebook, and replace `# use [command] here` with your answer_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! curl -s https://... > files.html # complete the URL of `download_files.html` and redirect to `files.html`\n",
    "! tail -n10 files.html # use tail to show last 10 lines of `files.html`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 1b: Chaining commands\n",
    "\n",
    "Instead of saving the output to file, then printing the last ten lines, you can execute a [sequence of commands together using pipes](http://swcarpentry.github.io/shell-novice/04-pipefilter/index.html). Using pipes, write a one line shell command to replace Question 1a.\n",
    "\n",
    "You can see from the output that the download progress is printed to screen. Something like this:\n",
    "```\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100  307k  100  307k    0     0   319k      0 --:--:-- --:--:-- --:--:--  319k\n",
    "```\n",
    "We don't want this to be a part of the html code, so use `curl`'s silent mode to eliminate the progress information.\n",
    "\n",
    "Save the result to a python variable named `lastlines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lastlines = ! curl -s https://... | tail -n10 # combine 1a into one command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2a: `grep`\n",
    "\n",
    "The `grep` command searches lines in a text file. Inspect what kinds of options are available using the help page: `grep --help`.\n",
    "\n",
    "Use `grep` to find lines with the substring `zip` and print first five lines using `head`. To avoid downloading the html page over and over again, use `files.html` as your input to `grep`.\n",
    "\n",
    "Save the resulting list of strings to a python variable named `ziplines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ziplines = ! grep ... files.html | head -n5 # look for lines containing string, `zip`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2b: Regular expression\n",
    "\n",
    "Note how the word `zip` doesn't just appear as a part of zip file names. We want our `grep` to match more specific pattern that looks like zip file names. The part of the html that becomes the download link is [`href`](https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_a_href). Let's search for filenames specified by `href`. \n",
    "\n",
    "Take, for example, the following line:\n",
    "```\n",
    "<a href=\"aqs_monitors.zip\">Monitor Listing</a> (353,283 Rows, 6,307 KB)<BR><BR>\n",
    "```\n",
    "There are many different and better ways of doing this. We will use `grep` and regular expression patterns to extract the links. A good way to learn regular expression is to know the basics and experiment.\n",
    "\n",
    "* [Quick reference of regular expression pattern](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) showing the basic building blocks\n",
    "* [Online utility for testing regular expression patterns](https://regexr.com)\n",
    "\n",
    "One quick way could be to find the pattern `href=\"[some file name]` to find all substrings containing links. Then filter the lines again that contain the word `zip`.\n",
    "\n",
    "Follow this sequence of commands using pipes:\n",
    "1. **Find lines with substring`zip` using `grep` and `files.html`.**  \n",
    "    First few lines would look similar to,\n",
    "```\n",
    "    the size of the (zipped) file, the number of data rows in the file,\n",
    "<a href=\"aqs_sites.zip\">Site Listing</a> (20,611 Rows, 982 KB)<BR>\n",
    "<a href=\"aqs_monitors.zip\">Monitor Listing</a> (353,283 Rows, 6,307 KB)<BR><BR>\n",
    "            <TD style=\"font-size:90%;text-align:center;\"><a href=\"annual_conc_by_monitor_2019.zip\">annual_conc_by_monitor_2019.zip</a><BR>51,707 Rows<BR>3,081 KB<BR>As of 2019-11-13</TD>\n",
    "```\n",
    "1. **Find lines with `grep` and a [regular expression](http://swcarpentry.github.io/shell-novice/07-find/index.html) pattern that matches URLs: `href=\"[some-url]\"`.**  \n",
    "    _The pattern `'href=\\\"[^\\\"]*'` would matches any `href=\"` and following string until another quote appears: e.g. `[^a]` in regular expression means not to match a. When using `grep` with regular expressions, it is easier to use `-E` option (for extended regular expression)._ After using `grep` option for extracting just the matching substrings (`-o`), you would get something similar to this:\n",
    "```\n",
    "href=\"aqs_sites.zip\n",
    "href=\"aqs_monitors.zip\n",
    "href=\"annual_conc_by_monitor_2019.zip\n",
    "href=\"annual_aqi_by_cbsa_2019.zip\n",
    "href=\"annual_aqi_by_county_2019.zip    \n",
    "```\n",
    "1. **Strip `href=\"` by using `sed`**  \n",
    "    We want just the URLs, so strip the unnecessary `href=` portion. Test using something like `echo \"hello there\" | sed 's/ll/rr/'`. For example, giving `'s/href=\\\"//g'` tells sed to replace `href=\"` with an empty string (`rr` is an empty string) globally (trailing `g`).  \n",
    "    After running `sed`, you would get something like this:\n",
    "```\n",
    "aqs_sites.zip\n",
    "aqs_monitors.zip\n",
    "annual_conc_by_monitor_2019.zip\n",
    "annual_aqi_by_cbsa_2019.zip\n",
    "annual_aqi_by_county_2019.zip    \n",
    "```\n",
    "    [_Refer to Unix Power Tools for more information (UCSB NetID required)_](https://proquest-safaribooksonline-com.proxy.library.ucsb.edu:9443/book/operating-systems-and-server-administration/unix/0596003307/34dot-the-sed-stream-editor/upt3_chp_34_sect_3_html)\n",
    "1. **Save the resulting list of strings (each string is a URL for a zip file) a python variable named `flist`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment one # at a time to see intermediate results\n",
    "flist = ! grep ... files.html \\\n",
    "#    | grep -o -E ... \\\n",
    "#    | sed ...\n",
    "\n",
    "print('\\n'.join(flist[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "In the previous question, we created a variable `flist` that contains 1705 file names by scraping a webpage. Hypothetically, you can loop through the list variable and download any file you need for your analysis.\n",
    "\n",
    "In this problem, we will work with a small portion of the data: `https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2019.zip`. Download this using `wget` and unzip it to find `annual_aqi_by_county_2019.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3a: Inspecting the Header\n",
    "\n",
    "Use `head` to print the header line of `annual_aqi_by_county_2019.csv` and use `sed` to print each column into one line. You can replace commas (`,`) with a newline character (`\\n`). Save to a python list variable named `aqiheader`. Each header name will become a list element.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aqiheader = ! head -n ... annual_aqi_by_county_2019.csv | sed 's/.../.../g' # your command\n",
    "\n",
    "print('\\n'.join(aqiheader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3b: Count Locations\n",
    "\n",
    "How many counties are there in each state? Each row is a county, so you can extract the `State` column and count how many rows there are for each state using `uniq`. Then, use `cat` to number the output lines. Save the result to a python variable `county_counts`. \n",
    "\n",
    "First few lines should look like this:\n",
    "```\n",
    "   1\t     16 \"Alabama\"\n",
    "   2\t      6 \"Alaska\"\n",
    "   3\t     13 \"Arizona\"\n",
    "   4\t     13 \"Arkansas\"\n",
    "   5\t     53 \"California\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment one line at a time to see what each line does\n",
    "county_counts = ! cut -d '...' -f ... annual_aqi_by_county_2019.csv \\\n",
    "#    | grep -v 'State' \\\n",
    "#    | uniq -c \\\n",
    "#    | cat -n\n",
    "\n",
    "\n",
    "print('\\n'.join(county_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3c: View CSV in Table\n",
    "\n",
    "Take first 5 lines of `annual_aqi_by_county_2019.csv`, extract first seven columns, and use `column` command to view data in a table. Assign the result to a python variable named `aqi_table`. The result looks like this:\n",
    "```\n",
    "\"State\"    \"County\"   \"Year\"  \"Days with AQI\"  \"Good Days\"  \"Moderate Days\"  \"Unhealthy for Sensitive Groups Days\"\n",
    "\"Alabama\"  \"Baldwin\"  2019    166              140          26               0\n",
    "\"Alabama\"  \"Clay\"     2019    63               58           5                0\n",
    "\"Alabama\"  \"Colbert\"  2019    171              161          10               0\n",
    "\"Alabama\"  \"DeKalb\"   2019    208              188          20               0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aqi_table = ! head -n ... annual_aqi_by_county_2019.csv | cut -d '...' -f ... | column -t -s','\n",
    "\n",
    "print('\\n'.join(aqi_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(filtering=False, pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1b": {
     "name": "q1b",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(lastlines)\n10",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> print(lastlines)\n['  </footer>', '  <script src=\"https://www.epa.gov/sites/all/libraries/template2/jquery.js\"></script>', '  <script src=\"https://www.epa.gov/sites/all/libraries/template2/js.js\"></script>', '  <script src=\"https://www.epa.gov/sites/all/modules/custom/epa_core/js/alert.js\"></script>', '  <!--[if lt IE 9]><script src=\"https://www.epa.gov/sites/all/themes/epa/js/ie.js\"></script><![endif]-->', '  <!-- REMOVE if not using -->', '  <!-- {LOCAL JAVASCRIPT} -->', '</body>', '</html>', '    ']\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> print(ziplines)\n['    the size of the (zipped) file, the number of data rows in the file,', '\\t<a href=\"aqs_sites.zip\">Site Listing</a> (20,797 Rows, 979 KB)<BR>', '\\t<a href=\"aqs_monitors.zip\">Monitor Listing</a> (359,940 Rows, 6,505 KB)<BR><BR>', '\\t\\t\\t\\t<TD style=\"font-size:90%;text-align:center;\"><a href=\"annual_conc_by_monitor_2022.zip\">annual_conc_by_monitor_2022.zip</a><BR>51,887 Rows<BR>2,882 KB<BR>As of 2022-11-14</TD>', '\\t\\t\\t\\t\\t<TD style=\"font-size:90%;text-align:center;\"><a href=\"annual_aqi_by_cbsa_2022.zip\">annual_aqi_by_cbsa_2022.zip</a><BR>496 Rows<BR>14 KB<BR>As of 2022-11-14</TD>']\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(flist)\n1875",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all([f.split('.')[-1] == 'zip' for f in flist])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> print(aqiheader)\n['\"State\"', '\"County\"', '\"Year\"', '\"Days with AQI\"', '\"Good Days\"', '\"Moderate Days\"', '\"Unhealthy for Sensitive Groups Days\"', '\"Unhealthy Days\"', '\"Very Unhealthy Days\"', '\"Hazardous Days\"', '\"Max AQI\"', '\"90th Percentile AQI\"', '\"Median AQI\"', '\"Days CO\"', '\"Days NO2\"', '\"Days Ozone\"', '\"Days SO2\"', '\"Days PM2.5\"', '\"Days PM10\"']\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> print(county_counts)\n['     1\\t     17 \"Alabama\"', '     2\\t      9 \"Alaska\"', '     3\\t     13 \"Arizona\"', '     4\\t     13 \"Arkansas\"', '     5\\t     53 \"California\"', '     6\\t     28 \"Colorado\"', '     7\\t      8 \"Connecticut\"', '     8\\t      2 \"Country Of Mexico\"', '     9\\t      3 \"Delaware\"', '    10\\t      1 \"District Of Columbia\"', '    11\\t     39 \"Florida\"', '    12\\t     29 \"Georgia\"', '    13\\t      4 \"Hawaii\"', '    14\\t     21 \"Idaho\"', '    15\\t     27 \"Illinois\"', '    16\\t     40 \"Indiana\"', '    17\\t     16 \"Iowa\"', '    18\\t     11 \"Kansas\"', '    19\\t     27 \"Kentucky\"', '    20\\t     23 \"Louisiana\"', '    21\\t     10 \"Maine\"', '    22\\t     17 \"Maryland\"', '    23\\t     13 \"Massachusetts\"', '    24\\t     28 \"Michigan\"', '    25\\t     21 \"Minnesota\"', '    26\\t     10 \"Mississippi\"', '    27\\t     24 \"Missouri\"', '    28\\t     19 \"Montana\"', '    29\\t      9 \"Nebraska\"', '    30\\t      9 \"Nevada\"', '    31\\t      7 \"New Hampshire\"', '    32\\t     16 \"New Jersey\"', '    33\\t     16 \"New Mexico\"', '    34\\t     31 \"New York\"', '    35\\t     38 \"North Carolina\"', '    36\\t     10 \"North Dakota\"', '    37\\t     42 \"Ohio\"', '    38\\t     22 \"Oklahoma\"', '    39\\t     24 \"Oregon\"', '    40\\t     41 \"Pennsylvania\"', '    41\\t     10 \"Puerto Rico\"', '    42\\t      3 \"Rhode Island\"', '    43\\t     18 \"South Carolina\"', '    44\\t     10 \"South Dakota\"', '    45\\t     23 \"Tennessee\"', '    46\\t     48 \"Texas\"', '    47\\t     15 \"Utah\"', '    48\\t      4 \"Vermont\"', '    49\\t      3 \"Virgin Islands\"', '    50\\t     34 \"Virginia\"', '    51\\t     31 \"Washington\"', '    52\\t     16 \"West Virginia\"', '    53\\t     28 \"Wisconsin\"', '    54\\t     18 \"Wyoming\"']\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(county_counts)\n54",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> print('\\n'.join(aqi_table))\n\"State\"    \"County\"   \"Year\"  \"Days with AQI\"  \"Good Days\"  \"Moderate Days\"  \"Unhealthy for Sensitive Groups Days\"\n\"Alabama\"  \"Baldwin\"  2019    271              237          34               0\n\"Alabama\"  \"Clay\"     2019    107              97           10               0\n\"Alabama\"  \"Colbert\"  2019    263              252          11               0\n\"Alabama\"  \"DeKalb\"   2019    361              324          37               0\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(aqi_table)\n5",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> print(aqi_table[0])\n\"State\"    \"County\"   \"Year\"  \"Days with AQI\"  \"Good Days\"  \"Moderate Days\"  \"Unhealthy for Sensitive Groups Days\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(','.join(aqi_table[1].split()).split(','))\n7",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
